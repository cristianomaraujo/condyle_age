# -*- coding: utf-8 -*-
"""CONDILO_IDADE_final_at√©18anos

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NaSwQyDMaTJmRdfFTb-muqU8xZ8zEzOK

**Data preparation**
"""

# @title
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
import matplotlib.pyplot as plt
import numpy as np

# @title
file_path = '/content/tabela_convertida5_ate_18.xlsx'
df = pd.read_excel(file_path)

# @title
df.columns

# @title
colunas = ['sex', 'age', 'larg_cond_med', 'alt_cond_med', 'alt_ramo_med',
       'larg_ramo_med', 'Co-GN',
       'Co-A']

df = df.loc[:, colunas]

coluna_grupo = df.pop('age')
df.insert(0, 'age', coluna_grupo)

# @title
df = df.dropna()

# @title
df.info()

"""**Model building**"""

# @title
X = df.iloc[:,1:]
y = df.iloc[:,0]

# @title
# supondo que y = df['sua_coluna']
Q1 = np.percentile(y, 25)
Q3 = np.percentile(y, 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

mask_outliers = (y < lower_bound) | (y > upper_bound)
df = df.loc[~mask_outliers].copy()  # cria um novo df sem outliers

# @title
#RANDOM STATE
RANDOM_STATE = 42

# @title
#Dataset split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=RANDOM_STATE, shuffle=True)

"""**GRADIENT BOOSTING REGRESSOR**"""

# @title
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de Gradient Boosting
gb_model = GradientBoostingRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperpar√¢metros
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10]
}

# Realizar a busca em grade com valida√ß√£o cruzada
grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_gb = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_gb_model_cl = GradientBoostingRegressor(**best_params_gb)

# Ajustar o modelo nos dados de treino
best_gb_model_cl.fit(X_train, y_train)
y_pred_test_gb2 = best_gb_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_gb = mean_squared_error(y_test, y_pred_test_gb2)
mae_gb = mean_absolute_error(y_test, y_pred_test_gb2)
rmse_gb = sqrt(mse_gb)
r2_gb = r2_score(y_test, y_pred_test_gb2)

# Fun√ß√£o para calcular as m√©tricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as m√©tricas de bootstrapping para os dados de teste
metrics_test_gb = bootstrap_metrics_test(best_gb_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_gb = ci95(metrics_test_gb['mse'])
rmse_ci_test_gb = ci95(metrics_test_gb['rmse'])
mae_ci_test_gb = ci95(metrics_test_gb['mae'])
r2_ci_test_gb = ci95(metrics_test_gb['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_gb['mse'])} (95% CI: {mse_ci_test_gb})")
print(f"Test Data RMSE: {np.mean(metrics_test_gb['rmse'])} (95% CI: {rmse_ci_test_gb})")
print(f"Test Data MAE: {np.mean(metrics_test_gb['mae'])} (95% CI: {mae_ci_test_gb})")
print(f"Test Data R2: {np.mean(metrics_test_gb['r2'])} (95% CI: {r2_ci_test_gb})")

# Realizando valida√ß√£o cruzada
kf_gb = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_gb2 = cross_val_predict(best_gb_model_cl, X_train, y_train, cv=kf_gb)

# Fun√ß√£o para calcular o IC95% na valida√ß√£o cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as m√©tricas de bootstrapping para valida√ß√£o cruzada
metrics_cv_gb = bootstrap_cv_metrics_regression(best_gb_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para valida√ß√£o cruzada
mse_ci_cv_gb = ci95(metrics_cv_gb['mse'])
rmse_ci_cv_gb = ci95(metrics_cv_gb['rmse'])
mae_ci_cv_gb = ci95(metrics_cv_gb['mae'])
r2_ci_cv_gb = ci95(metrics_cv_gb['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_gb['mse'])} (95% CI: {mse_ci_cv_gb})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_gb['rmse'])} (95% CI: {rmse_ci_cv_gb})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_gb['mae'])} (95% CI: {mae_ci_cv_gb})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_gb['r2'])} (95% CI: {r2_ci_cv_gb})")

print("Best Parameters:", grid_search.best_params_)

"""**LINEAR REGRESSION**"""

# @title
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de regress√£o
regression_model = LinearRegression()

# Definir a grade de hiperpar√¢metros
param_grid = {
    'fit_intercept': [True, False],
    'copy_X': [True, False],
    'n_jobs': [-1, 1],
    'positive': [True, False]
}

# Realizar a busca em grade com valida√ß√£o cruzada
grid_search = GridSearchCV(regression_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_linear = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_linear_model_cl = LinearRegression(**best_params_linear)

# Ajustar o modelo nos dados de treino
best_linear_model_cl.fit(X_train, y_train)
y_pred_test_linear2 = best_linear_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_linear = mean_squared_error(y_test, y_pred_test_linear2)
mae_linear = mean_absolute_error(y_test, y_pred_test_linear2)
rmse_linear = sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_test_linear2)

# Fun√ß√£o para calcular as m√©tricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as m√©tricas de bootstrapping para os dados de teste
metrics_test_linear = bootstrap_metrics_test(best_linear_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_linear = ci95(metrics_test_linear['mse'])
rmse_ci_test_linear = ci95(metrics_test_linear['rmse'])
mae_ci_test_linear = ci95(metrics_test_linear['mae'])
r2_ci_test_linear = ci95(metrics_test_linear['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_linear['mse'])} (95% CI: {mse_ci_test_linear})")
print(f"Test Data RMSE: {np.mean(metrics_test_linear['rmse'])} (95% CI: {rmse_ci_test_linear})")
print(f"Test Data MAE: {np.mean(metrics_test_linear['mae'])} (95% CI: {mae_ci_test_linear})")
print(f"Test Data R2: {np.mean(metrics_test_linear['r2'])} (95% CI: {r2_ci_test_linear})")

# Realizando valida√ß√£o cruzada
kf_linear = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_linear2 = cross_val_predict(best_linear_model_cl, X_train, y_train, cv=kf_linear)

# Fun√ß√£o para calcular o IC95% na valida√ß√£o cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as m√©tricas de bootstrapping para valida√ß√£o cruzada
metrics_cv_linear = bootstrap_cv_metrics_regression(best_linear_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para valida√ß√£o cruzada
mse_ci_cv_linear = ci95(metrics_cv_linear['mse'])
rmse_ci_cv_linear = ci95(metrics_cv_linear['rmse'])
mae_ci_cv_linear = ci95(metrics_cv_linear['mae'])
r2_ci_cv_linear = ci95(metrics_cv_linear['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_linear['mse'])} (95% CI: {mse_ci_cv_linear})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_linear['rmse'])} (95% CI: {rmse_ci_cv_linear})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_linear['mae'])} (95% CI: {mae_ci_cv_linear})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_linear['r2'])} (95% CI: {r2_ci_cv_linear})")

# @title
coefficients = best_linear_model_cl.coef_

feature_importance_lr = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance_lr = feature_importance_lr.sort_values('Importance', ascending=True)

colors = plt.cm.viridis(np.linspace(0.2, 1, len(feature_importance_lr)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_lr['Feature'], feature_importance_lr['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()
plt.show()

"""**SVM**"""

# @title
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo SVM
svm_model = SVR()

# Definir a grade de hiperpar√¢metros
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'degree': [2, 3, 4]
}

# Realizar a busca em grade com valida√ß√£o cruzada
grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_svm = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_svm_model_cl = SVR(**best_params_svm)

# Ajustar o modelo nos dados de treino
best_svm_model_cl.fit(X_train, y_train)
y_pred_test_svm2 = best_svm_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_svm = mean_squared_error(y_test, y_pred_test_svm2)
mae_svm = mean_absolute_error(y_test, y_pred_test_svm2)
rmse_svm = sqrt(mse_svm)
r2_svm = r2_score(y_test, y_pred_test_svm2)

# Fun√ß√£o para calcular as m√©tricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as m√©tricas de bootstrapping para os dados de teste
metrics_test_svm = bootstrap_metrics_test(best_svm_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_svm = ci95(metrics_test_svm['mse'])
rmse_ci_test_svm = ci95(metrics_test_svm['rmse'])
mae_ci_test_svm = ci95(metrics_test_svm['mae'])
r2_ci_test_svm = ci95(metrics_test_svm['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_svm['mse'])} (95% CI: {mse_ci_test_svm})")
print(f"Test Data RMSE: {np.mean(metrics_test_svm['rmse'])} (95% CI: {rmse_ci_test_svm})")
print(f"Test Data MAE: {np.mean(metrics_test_svm['mae'])} (95% CI: {mae_ci_test_svm})")
print(f"Test Data R2: {np.mean(metrics_test_svm['r2'])} (95% CI: {r2_ci_test_svm})")

# Realizando valida√ß√£o cruzada
kf_svm = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_svm2 = cross_val_predict(best_svm_model_cl, X_train, y_train, cv=kf_svm)

# Fun√ß√£o para calcular o IC95% na valida√ß√£o cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as m√©tricas de bootstrapping para valida√ß√£o cruzada
metrics_cv_svm = bootstrap_cv_metrics_regression(best_svm_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para valida√ß√£o cruzada
mse_ci_cv_svm = ci95(metrics_cv_svm['mse'])
rmse_ci_cv_svm = ci95(metrics_cv_svm['rmse'])
mae_ci_cv_svm = ci95(metrics_cv_svm['mae'])
r2_ci_cv_svm = ci95(metrics_cv_svm['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_svm['mse'])} (95% CI: {mse_ci_cv_svm})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_svm['rmse'])} (95% CI: {rmse_ci_cv_svm})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_svm['mae'])} (95% CI: {mae_ci_cv_svm})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_svm['r2'])} (95% CI: {r2_ci_cv_svm})")

"""**KNN**"""

# @title
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo KNN
knn_model = KNeighborsRegressor()

# Definir a grade de hiperpar√¢metros
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # p=1 para dist√¢ncia de Manhattan, p=2 para dist√¢ncia Euclidiana
}

# Realizar a busca em grade com valida√ß√£o cruzada
grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_knn = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_knn_model_cl = KNeighborsRegressor(**best_params_knn)

# Ajustar o modelo nos dados de treino
best_knn_model_cl.fit(X_train, y_train)
y_pred_test_knn2 = best_knn_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_knn = mean_squared_error(y_test, y_pred_test_knn2)
mae_knn = mean_absolute_error(y_test, y_pred_test_knn2)
rmse_knn = sqrt(mse_knn)
r2_knn = r2_score(y_test, y_pred_test_knn2)

# Fun√ß√£o para calcular as m√©tricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as m√©tricas de bootstrapping para os dados de teste
metrics_test_knn = bootstrap_metrics_test(best_knn_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_knn = ci95(metrics_test_knn['mse'])
rmse_ci_test_knn = ci95(metrics_test_knn['rmse'])
mae_ci_test_knn = ci95(metrics_test_knn['mae'])
r2_ci_test_knn = ci95(metrics_test_knn['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_knn['mse'])} (95% CI: {mse_ci_test_knn})")
print(f"Test Data RMSE: {np.mean(metrics_test_knn['rmse'])} (95% CI: {rmse_ci_test_knn})")
print(f"Test Data MAE: {np.mean(metrics_test_knn['mae'])} (95% CI: {mae_ci_test_knn})")
print(f"Test Data R2: {np.mean(metrics_test_knn['r2'])} (95% CI: {r2_ci_test_knn})")

# Realizando valida√ß√£o cruzada
kf_knn = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_knn2 = cross_val_predict(best_knn_model_cl, X_train, y_train, cv=kf_knn)

# Fun√ß√£o para calcular o IC95% na valida√ß√£o cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as m√©tricas de bootstrapping para valida√ß√£o cruzada
metrics_cv_knn = bootstrap_cv_metrics_regression(best_knn_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para valida√ß√£o cruzada
mse_ci_cv_knn = ci95(metrics_cv_knn['mse'])
rmse_ci_cv_knn = ci95(metrics_cv_knn['rmse'])
mae_ci_cv_knn = ci95(metrics_cv_knn['mae'])
r2_ci_cv_knn = ci95(metrics_cv_knn['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_knn['mse'])} (95% CI: {mse_ci_cv_knn})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_knn['rmse'])} (95% CI: {rmse_ci_cv_knn})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_knn['mae'])} (95% CI: {mae_ci_cv_knn})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_knn['r2'])} (95% CI: {r2_ci_cv_knn})")

"""**RANDOM FOREST**"""

# @title
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Random Forest
rf_model = RandomForestRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperpar√¢metros
param_grid = {
    'n_estimators': [50, 100, 200],  # N√∫mero de √°rvores na floresta
    'max_depth': [None, 10, 20],  # Profundidade m√°xima das √°rvores
    'min_samples_split': [2, 5, 10],  # N√∫mero m√≠nimo de amostras necess√°rias para dividir um n√≥ interno
    'min_samples_leaf': [1, 2, 4],  # N√∫mero m√≠nimo de amostras necess√°rias para ser uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # N√∫mero de features a serem consideradas para cada split
}

# Realizar a busca em grade com valida√ß√£o cruzada
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_rf = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_rf_model_cl = RandomForestRegressor(**best_params_rf, random_state=42)

# Ajustar o modelo nos dados de treino
best_rf_model_cl.fit(X_train, y_train)
y_pred_test_rf2 = best_rf_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_rf = mean_squared_error(y_test, y_pred_test_rf2)
mae_rf = mean_absolute_error(y_test, y_pred_test_rf2)
rmse_rf = sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_test_rf2)

# Fun√ß√£o para calcular as m√©tricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as m√©tricas de bootstrapping para os dados de teste
metrics_test_rf = bootstrap_metrics_test(best_rf_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_rf = ci95(metrics_test_rf['mse'])
rmse_ci_test_rf = ci95(metrics_test_rf['rmse'])
mae_ci_test_rf = ci95(metrics_test_rf['mae'])
r2_ci_test_rf = ci95(metrics_test_rf['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_rf['mse'])} (95% CI: {mse_ci_test_rf})")
print(f"Test Data RMSE: {np.mean(metrics_test_rf['rmse'])} (95% CI: {rmse_ci_test_rf})")
print(f"Test Data MAE: {np.mean(metrics_test_rf['mae'])} (95% CI: {mae_ci_test_rf})")
print(f"Test Data R2: {np.mean(metrics_test_rf['r2'])} (95% CI: {r2_ci_test_rf})")

# Realizando valida√ß√£o cruzada
kf_rf = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_rf2 = cross_val_predict(best_rf_model_cl, X_train, y_train, cv=kf_rf)

# Fun√ß√£o para calcular o IC95% na valida√ß√£o cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as m√©tricas de bootstrapping para valida√ß√£o cruzada
metrics_cv_rf = bootstrap_cv_metrics_regression(best_rf_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para valida√ß√£o cruzada
mse_ci_cv_rf = ci95(metrics_cv_rf['mse'])
rmse_ci_cv_rf = ci95(metrics_cv_rf['rmse'])
mae_ci_cv_rf = ci95(metrics_cv_rf['mae'])
r2_ci_cv_rf = ci95(metrics_cv_rf['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_rf['mse'])} (95% CI: {mse_ci_cv_rf})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_rf['rmse'])} (95% CI: {rmse_ci_cv_rf})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_rf['mae'])} (95% CI: {mae_ci_cv_rf})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_rf['r2'])} (95% CI: {r2_ci_cv_rf})")

"""**Adaboost Regressor**"""

# @title
from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo AdaBoost
ada_model = AdaBoostRegressor(random_state=42)

# Definir a grade de hiperpar√¢metros
param_grid = {
    'n_estimators': [50, 100, 200],  # N√∫mero de estimadores base
    'learning_rate': [0.01, 0.1, 1.0],  # Taxa de aprendizado
    'loss': ['linear', 'square', 'exponential']  # Fun√ß√£o de perda
}

# Realizar a busca em grade com valida√ß√£o cruzada
grid_search = GridSearchCV(ada_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_ada = grid_search.best_params_
print("Best Parameters:", best_params_ada)

# Treinar o melhor modelo encontrado
best_ada_model = AdaBoostRegressor(**best_params_ada, random_state=42)

# Ajustar o modelo nos dados de treino
best_ada_model.fit(X_train, y_train)
y_pred_test_ada = best_ada_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_ada = mean_squared_error(y_test, y_pred_test_ada)
mae_ada = mean_absolute_error(y_test, y_pred_test_ada)
rmse_ada = sqrt(mse_ada)
r2_ada = r2_score(y_test, y_pred_test_ada)

# Fun√ß√£o para calcular as m√©tricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as m√©tricas de bootstrapping para os dados de teste
metrics_test_ada = bootstrap_metrics_test(best_ada_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_ada = ci95(metrics_test_ada['mse'])
rmse_ci_test_ada = ci95(metrics_test_ada['rmse'])
mae_ci_test_ada = ci95(metrics_test_ada['mae'])
r2_ci_test_ada = ci95(metrics_test_ada['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_ada['mse'])} (95% CI: {mse_ci_test_ada})")
print(f"Test Data RMSE: {np.mean(metrics_test_ada['rmse'])} (95% CI: {rmse_ci_test_ada})")
print(f"Test Data MAE: {np.mean(metrics_test_ada['mae'])} (95% CI: {mae_ci_test_ada})")
print(f"Test Data R2: {np.mean(metrics_test_ada['r2'])} (95% CI: {r2_ci_test_ada})")

# Realizando valida√ß√£o cruzada
kf_ada = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_ada = cross_val_predict(best_ada_model, X_train, y_train, cv=kf_ada)

# Fun√ß√£o para calcular o IC95% na valida√ß√£o cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as m√©tricas de bootstrapping para valida√ß√£o cruzada
metrics_cv_ada = bootstrap_cv_metrics_regression(best_ada_model, X_train, y_train, cv=5)

# Calcular os IC95% para valida√ß√£o cruzada
mse_ci_cv_ada = ci95(metrics_cv_ada['mse'])
rmse_ci_cv_ada = ci95(metrics_cv_ada['rmse'])
mae_ci_cv_ada = ci95(metrics_cv_ada['mae'])
r2_ci_cv_ada = ci95(metrics_cv_ada['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_ada['mse'])} (95% CI: {mse_ci_cv_ada})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_ada['rmse'])} (95% CI: {rmse_ci_cv_ada})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_ada['mae'])} (95% CI: {mae_ci_cv_ada})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_ada['r2'])} (95% CI: {r2_ci_cv_ada})")

"""**Decision Tree**"""

# @title
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Decision Tree
dt_model = DecisionTreeRegressor(random_state=42)

# Definir a grade de hiperpar√¢metros
param_grid = {
    'max_depth': [None, 10, 20, 30],  # Profundidade m√°xima da √°rvore
    'min_samples_split': [2, 5, 10],  # N√∫mero m√≠nimo de amostras para dividir um n√≥
    'min_samples_leaf': [1, 2, 4],  # N√∫mero m√≠nimo de amostras em uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # N√∫mero de features a serem consideradas para cada split
}

# Realizar a busca em grade com valida√ß√£o cruzada
grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_dt = grid_search.best_params_
print("Best Parameters:", best_params_dt)

# Treinar o melhor modelo encontrado
best_dt_model = DecisionTreeRegressor(**best_params_dt, random_state=42)

# Ajustar o modelo nos dados de treino
best_dt_model.fit(X_train, y_train)
y_pred_test_dt = best_dt_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_dt = mean_squared_error(y_test, y_pred_test_dt)
mae_dt = mean_absolute_error(y_test, y_pred_test_dt)
rmse_dt = sqrt(mse_dt)
r2_dt = r2_score(y_test, y_pred_test_dt)

# Fun√ß√£o para calcular as m√©tricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as m√©tricas de bootstrapping para os dados de teste
metrics_test_dt = bootstrap_metrics_test(best_dt_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_dt = ci95(metrics_test_dt['mse'])
rmse_ci_test_dt = ci95(metrics_test_dt['rmse'])
mae_ci_test_dt = ci95(metrics_test_dt['mae'])
r2_ci_test_dt = ci95(metrics_test_dt['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_dt['mse'])} (95% CI: {mse_ci_test_dt})")
print(f"Test Data RMSE: {np.mean(metrics_test_dt['rmse'])} (95% CI: {rmse_ci_test_dt})")
print(f"Test Data MAE: {np.mean(metrics_test_dt['mae'])} (95% CI: {mae_ci_test_dt})")
print(f"Test Data R2: {np.mean(metrics_test_dt['r2'])} (95% CI: {r2_ci_test_dt})")

# Realizando valida√ß√£o cruzada
kf_dt = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_dt = cross_val_predict(best_dt_model, X_train, y_train, cv=kf_dt)

# Fun√ß√£o para calcular o IC95% na valida√ß√£o cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as m√©tricas de bootstrapping para valida√ß√£o cruzada
metrics_cv_dt = bootstrap_cv_metrics_regression(best_dt_model, X_train, y_train, cv=5)

# Calcular os IC95% para valida√ß√£o cruzada
mse_ci_cv_dt = ci95(metrics_cv_dt['mse'])
rmse_ci_cv_dt = ci95(metrics_cv_dt['rmse'])
mae_ci_cv_dt = ci95(metrics_cv_dt['mae'])
r2_ci_cv_dt = ci95(metrics_cv_dt['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_dt['mse'])} (95% CI: {mse_ci_cv_dt})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_dt['rmse'])} (95% CI: {rmse_ci_cv_dt})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_dt['mae'])} (95% CI: {mae_ci_cv_dt})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_dt['r2'])} (95% CI: {r2_ci_cv_dt})")

"""**MLP Regressor**"""

# @title
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo MLP
mlp_model = MLPRegressor(random_state=42, max_iter=5000)

# Definir a grade de hiperpar√¢metros
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Diferentes arquiteturas de camadas ocultas
    'activation': ['relu', 'tanh'],  # Fun√ß√µes de ativa√ß√£o
    'solver': ['adam', 'lbfgs'],  # Otimizadores
    'alpha': [1e-4, 1e-3],  # Regulariza√ß√£o L2
    'learning_rate': ['constant', 'adaptive']  # Taxa de aprendizado
}

# Realizar a busca em grade com valida√ß√£o cruzada
grid_search = GridSearchCV(mlp_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)
best_params_mlp = grid_search.best_params_
print("Best Parameters:", best_params_mlp)

# Treinar o melhor modelo encontrado
best_mlp_model = MLPRegressor(**best_params_mlp, random_state=42, max_iter=5000)

# Ajustar o modelo nos dados de treino
best_mlp_model.fit(X_train, y_train)
y_pred_test_mlp = best_mlp_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_mlp = mean_squared_error(y_test, y_pred_test_mlp)
mae_mlp = mean_absolute_error(y_test, y_pred_test_mlp)
rmse_mlp = sqrt(mse_mlp)
r2_mlp = r2_score(y_test, y_pred_test_mlp)

# Fun√ß√£o para calcular as m√©tricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as m√©tricas de bootstrapping para os dados de teste
metrics_test_mlp = bootstrap_metrics_test(best_mlp_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_mlp = ci95(metrics_test_mlp['mse'])
rmse_ci_test_mlp = ci95(metrics_test_mlp['rmse'])
mae_ci_test_mlp = ci95(metrics_test_mlp['mae'])
r2_ci_test_mlp = ci95(metrics_test_mlp['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_mlp['mse'])} (95% CI: {mse_ci_test_mlp})")
print(f"Test Data RMSE: {np.mean(metrics_test_mlp['rmse'])} (95% CI: {rmse_ci_test_mlp})")
print(f"Test Data MAE: {np.mean(metrics_test_mlp['mae'])} (95% CI: {mae_ci_test_mlp})")
print(f"Test Data R2: {np.mean(metrics_test_mlp['r2'])} (95% CI: {r2_ci_test_mlp})")

# Realizando valida√ß√£o cruzada
kf_mlp = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_mlp = cross_val_predict(best_mlp_model, X_train, y_train, cv=kf_mlp)

# Fun√ß√£o para calcular o IC95% na valida√ß√£o cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=500, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as m√©tricas de bootstrapping para valida√ß√£o cruzada
metrics_cv_mlp = bootstrap_cv_metrics_regression(best_mlp_model, X_train, y_train, cv=5)

# Calcular os IC95% para valida√ß√£o cruzada
mse_ci_cv_mlp = ci95(metrics_cv_mlp['mse'])
rmse_ci_cv_mlp = ci95(metrics_cv_mlp['rmse'])
mae_ci_cv_mlp = ci95(metrics_cv_mlp['mae'])
r2_ci_cv_mlp = ci95(metrics_cv_mlp['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_mlp['mse'])} (95% CI: {mse_ci_cv_mlp})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_mlp['rmse'])} (95% CI: {rmse_ci_cv_mlp})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_mlp['mae'])} (95% CI: {mae_ci_cv_mlp})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_mlp['r2'])} (95% CI: {r2_ci_cv_mlp})")

"""**SCATTER PLOT**"""

# @title
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def calculate_metrics_with_ci(y_true, y_pred, n_iterations=1000, random_state=42):
    """
    Calculates MAE, RMSE, and R¬≤ metrics along with 95% confidence intervals
    using bootstrap resampling.
    """
    np.random.seed(random_state)
    mae_list, rmse_list, r2_list = [], [], []

    for _ in range(n_iterations):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred_resampled = y_pred[indices]

        mae_list.append(mean_absolute_error(y_true_resampled, y_pred_resampled))
        rmse_list.append(np.sqrt(mean_squared_error(y_true_resampled, y_pred_resampled)))
        r2_list.append(r2_score(y_true_resampled, y_pred_resampled))

    metrics = {
        "MAE": (np.mean(mae_list), np.percentile(mae_list, [2.5, 97.5])),
        "RMSE": (np.mean(rmse_list), np.percentile(rmse_list, [2.5, 97.5])),
        "R¬≤":  (np.mean(r2_list), np.percentile(r2_list, [2.5, 97.5]))
    }
    return metrics

# Example dictionary with models and predictions
models = {
    "Linear Regression": y_pred_test_linear2,
    "Gradient Boosting": y_pred_test_gb2,
    "SVR": y_pred_test_svm2,
    "KNeighbors": y_pred_test_knn2,
    "Random Forest": y_pred_test_rf2,
    "MLP": y_pred_test_mlp,
    "Decision Tree": y_pred_test_dt,
    "AdaBoost": y_pred_test_ada
}

y_test_array = np.array(y_test)

# Figure with generous size
fig, axs = plt.subplots(3, 3, figsize=(24, 18))
axs = axs.flatten()  # Flatten to easily iterate

for i, (model_name, y_pred) in enumerate(models.items()):
    y_pred_array = np.array(y_pred)

    # Calculate metrics and confidence intervals
    metrics = calculate_metrics_with_ci(y_test_array, y_pred_array)
    mae, mae_ci = metrics["MAE"]
    rmse, rmse_ci = metrics["RMSE"]
    r2, r2_ci = metrics["R¬≤"]

    # Determine min and max for axes and identity line
    x_min, x_max = y_test_array.min(), y_test_array.max()
    y_min, y_max = y_pred_array.min(), y_pred_array.max()
    overall_min = min(x_min, y_min)
    overall_max = max(x_max, y_max)

    # Scatter plot
    axs[i].scatter(
        y_test_array,
        y_pred_array,
        color='royalblue',
        alpha=0.6,
        s=100,
        label='Predicted Values'
    )
    # Identity line
    axs[i].plot(
        [overall_min, overall_max],
        [overall_min, overall_max],
        'k--',
        label='y = x (Identity)'
    )

    axs[i].set_title(f"{model_name}: Actual vs Predicted", fontsize=14, pad=10)
    axs[i].set_xlabel("Actual Values", fontsize=12)
    axs[i].set_ylabel("Predicted Values", fontsize=12)
    axs[i].grid(True, alpha=0.3)

    # Set tight axis limits so data + identity line are fully visible
    axs[i].set_xlim([overall_min, overall_max])
    axs[i].set_ylim([overall_min, overall_max])

    # Place metrics in the bottom-right corner
    axs[i].text(
        0.95,
        0.05,
        f"MAE: {mae:.2f} [{mae_ci[0]:.2f}, {mae_ci[1]:.2f}]\n"
        f"RMSE: {rmse:.2f} [{rmse_ci[0]:.2f}, {rmse_ci[1]:.2f}]\n"
        f"R¬≤: {r2:.2f} [{r2_ci[0]:.2f}, {r2_ci[1]:.2f}]",
        transform=axs[i].transAxes,
        fontsize=10,
        verticalalignment='bottom',
        horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.4', facecolor='ivory', alpha=0.8)
    )

    axs[i].legend(fontsize=10, loc='upper left')

# Remove extra subplots if fewer than 9 models
for j in range(len(models), len(axs)):
    fig.delaxes(axs[j])

plt.tight_layout()
plt.show()

# @title
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def calculate_metrics_with_ci(y_true, y_pred, n_iterations=1000, random_state=42):
    """
    Calculates MAE, RMSE, and R¬≤ metrics along with 95% confidence intervals
    using bootstrap resampling.
    """
    rng = np.random.default_rng(random_state)
    mae_list, rmse_list, r2_list = [], [], []

    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)

    for _ in range(n_iterations):
        idx = rng.integers(0, len(y_true), size=len(y_true))
        yt = y_true[idx]
        yp = y_pred[idx]
        mae_list.append(mean_absolute_error(yt, yp))
        rmse_list.append(np.sqrt(mean_squared_error(yt, yp)))
        r2_list.append(r2_score(yt, yp))

    return {
        "MAE": (float(np.mean(mae_list)), np.percentile(mae_list, [2.5, 97.5])),
        "RMSE": (float(np.mean(rmse_list)), np.percentile(rmse_list, [2.5, 97.5])),
        "R¬≤": (float(np.mean(r2_list)), np.percentile(r2_list, [2.5, 97.5])),
    }

# ---------------------------------------------------------
# Models and predictions (use your existing variables)
# ---------------------------------------------------------
models = {
    "Linear Regression": y_pred_test_linear2,
    "Gradient Boosting": y_pred_test_gb2,
    "SVR": y_pred_test_svm2,
    "KNeighbors": y_pred_test_knn2,
    "Random Forest": y_pred_test_rf2,
    "MLP": y_pred_test_mlp,
    "Decision Tree": y_pred_test_dt,
    "AdaBoost": y_pred_test_ada
}

y_test_array = np.asarray(y_test)

# ---------------------------------------------------------
# Visual style (new colors)
# ---------------------------------------------------------
density_cmap = "magma"     # KDE background colormap
point_color = "#1f77b4"    # blue
edge_color = "#2c2c2c"     # dark gray

# ---------------------------------------------------------
# Global axis limits (consistent across all subplots) + padding
# ---------------------------------------------------------
all_preds = np.concatenate([np.asarray(v) for v in models.values()])
global_min = float(min(y_test_array.min(), all_preds.min()))
global_max = float(max(y_test_array.max(), all_preds.max()))
pad = 0.10 * (global_max - global_min)  # 10% padding
plot_min = global_min - pad
plot_max = global_max + pad

# ---------------------------------------------------------
# Figure
# ---------------------------------------------------------
fig, axs = plt.subplots(3, 3, figsize=(24, 18))
axs = axs.flatten()

for i, (model_name, y_pred) in enumerate(models.items()):
    y_pred_array = np.asarray(y_pred)

    # Metrics with CI
    metrics = calculate_metrics_with_ci(y_test_array, y_pred_array)
    mae, mae_ci = metrics["MAE"]
    rmse, rmse_ci = metrics["RMSE"]
    r2, r2_ci = metrics["R¬≤"]

    # 2D KDE for density background
    xy = np.vstack([y_test_array, y_pred_array])
    kde = gaussian_kde(xy)

    xi = np.linspace(plot_min, plot_max, 220)
    yi = np.linspace(plot_min, plot_max, 220)
    X, Y = np.meshgrid(xi, yi)
    Z = kde(np.vstack([X.ravel(), Y.ravel()])).reshape(X.shape)

    contourf = axs[i].contourf(
        X, Y, Z,
        levels=18,
        cmap=density_cmap,
        alpha=0.25
    )

    # Subtle halo (optional). Remove this block if you want it even cleaner.
    axs[i].scatter(
        y_test_array, y_pred_array,
        s=90, alpha=0.05,
        color=point_color,
        edgecolors="none"
    )

    # Main scatter (smaller points)
    axs[i].scatter(
        y_test_array, y_pred_array,
        s=38, alpha=0.75,
        facecolors=point_color,
        edgecolors=edge_color,
        linewidths=0.5,
        label="Predicted Values"
    )

    # Identity line only (trend line removed)
    axs[i].plot(
        [plot_min, plot_max],
        [plot_min, plot_max],
        linestyle="--",
        linewidth=1.6,
        color=edge_color,
        label="y = x (Identity)"
    )

    axs[i].set_title(f"{model_name}: Actual vs Predicted", fontsize=14, pad=10)
    axs[i].set_xlabel("Actual Values", fontsize=12)
    axs[i].set_ylabel("Predicted Values", fontsize=12)
    axs[i].grid(True, alpha=0.25)

    # Limits + 1:1 aspect
    axs[i].set_xlim(plot_min, plot_max)
    axs[i].set_ylim(plot_min, plot_max)
    axs[i].set_aspect("equal", adjustable="box")

    # Colorbar per subplot
    cbar = fig.colorbar(contourf, ax=axs[i], fraction=0.046, pad=0.03)
    cbar.set_label("Density", fontsize=10)

    # Metrics box
    axs[i].text(
        0.97, 0.05,
        f"MAE: {mae:.2f} [{mae_ci[0]:.2f}, {mae_ci[1]:.2f}]\n"
        f"RMSE: {rmse:.2f} [{rmse_ci[0]:.2f}, {rmse_ci[1]:.2f}]\n"
        f"R¬≤: {r2:.2f} [{r2_ci[0]:.2f}, {r2_ci[1]:.2f}]",
        transform=axs[i].transAxes,
        fontsize=10,
        va="bottom",
        ha="right",
        bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.85)
    )

    axs[i].legend(fontsize=10, loc="upper left")

# Remove extra subplots if fewer than 9 models
for j in range(len(models), len(axs)):
    fig.delaxes(axs[j])

plt.tight_layout()
plt.savefig("scatter_reg.jpg", dpi=600, bbox_inches="tight")
plt.show()

# @title
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

# Previs√µes dos modelos
y_pred_gb = best_gb_model_cl.predict(X_test)
y_pred_lr = best_linear_model_cl.predict(X_test)
y_pred_svm = best_svm_model_cl.predict(X_test)
y_pred_knn = best_knn_model_cl.predict(X_test)
y_pred_rf = best_rf_model_cl.predict(X_test)
y_pred_mlp = best_mlp_model.predict(X_test)
y_pred_dt = best_dt_model.predict(X_test)
y_pred_ada = best_ada_model.predict(X_test)

# Fun√ß√£o para bootstrap
def compare_mae_bootstrap(y_true, y_pred1, y_pred2, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    diffs = []

    for _ in range(n_iterations):
        idx = np.random.choice(len(y_true), len(y_true), replace=True)
        mae1 = mean_absolute_error(y_true[idx], y_pred1[idx])
        mae2 = mean_absolute_error(y_true[idx], y_pred2[idx])
        diffs.append(mae1 - mae2)

    return np.mean(diffs), np.percentile(diffs, [2.5, 97.5])

# Organiza√ß√£o
model_names = ["gb", "lr", "svm", "knn", "rf", "mlp", "dt", "ada"]
y_preds = {
    "gb": y_pred_gb,
    "lr": y_pred_lr,
    "svm": y_pred_svm,
    "knn": y_pred_knn,
    "rf": y_pred_rf,
    "mlp": y_pred_mlp,
    "dt": y_pred_dt,
    "ada": y_pred_ada
}

model_pairs = [(m1, m2) for i, m1 in enumerate(model_names) for m2 in model_names[i+1:]]
mean_diffs, cis = [], []

for m1, m2 in model_pairs:
    md, ci = compare_mae_bootstrap(
        np.array(y_test),
        np.array(y_preds[m1]),
        np.array(y_preds[m2])
    )
    mean_diffs.append(md)
    cis.append(ci)

lower = np.array([c[0] for c in cis])
upper = np.array([c[1] for c in cis])

labels = [f"{m1.upper()} vs {m2.upper()}" for m1, m2 in model_pairs]

# üé® NOVAS CORES
POS_FILL = "#4c72b0"
NEG_FILL = "#c44e52"
POS_EDGE = "#2c2c2c"
NEG_EDGE = "#7f1d1d"

bar_colors = [POS_FILL if md >= 0 else NEG_FILL for md in mean_diffs]
edge_colors = [POS_EDGE if md >= 0 else NEG_EDGE for md in mean_diffs]

# Plot
plt.figure(figsize=(16, 10))
plt.bar(
    labels,
    mean_diffs,
    yerr=[np.abs(lower - mean_diffs), np.abs(upper - mean_diffs)],
    capsize=8,
    color=bar_colors,
    edgecolor=edge_colors
)

plt.axhline(0, color="#2c2c2c", linestyle="--", linewidth=1.5)
plt.xticks(rotation=45, ha="right", fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel("Model Pairs", fontsize=14)
plt.ylabel("Difference in MAE", fontsize=14)
plt.title("Difference in MAE between Model Pairs with 95% CI", fontsize=16)
plt.grid(axis="y", linestyle="--", alpha=0.35)

# Legenda simples e coerente
legend_handles = [
    plt.Line2D([0], [0], marker="s", color="w",
               markerfacecolor=POS_FILL, markersize=12, label="Positive Difference"),
    plt.Line2D([0], [0], marker="s", color="w",
               markerfacecolor=NEG_FILL, markersize=12, label="Negative Difference")
]
plt.legend(handles=legend_handles, loc="upper left", fontsize=12)

# Texto explicativo dos modelos
legend_text = {
    "GB": "Gradient Boosting Regressor",
    "LR": "Linear Regression",
    "SVM": "Support Vector Machine Regressor",
    "KNN": "K-Nearest Neighbors Regressor",
    "RF": "Random Forest Regressor",
    "MLP": "Multi-layer Perceptron Regressor",
    "DT": "Decision Tree Regressor",
    "ADA": "AdaBoost Regressor"
}

plt.text(
    0.73, 0.98,
    "\n".join(f"{k}: {v}" for k, v in legend_text.items()),
    transform=plt.gca().transAxes,
    fontsize=12,
    va="top",
    bbox=dict(facecolor="white", alpha=0.85)
)

plt.tight_layout()
plt.savefig("Meandifference_reg.jpg", dpi=600, bbox_inches="tight")
plt.show()

# @title
import numpy as np
import matplotlib.pyplot as plt

X = df.drop('age', axis=1)
features = X.columns

imp_features_gb = best_gb_model_cl.feature_importances_
imp_features_lr = np.abs(best_linear_model_cl.coef_.ravel())
imp_features_dt = best_dt_model.feature_importances_
imp_features_rf = best_rf_model_cl.feature_importances_
imp_features_ada = best_ada_model.feature_importances_

sorted_indices_gb = np.argsort(imp_features_gb)
sorted_indices_lr = np.argsort(imp_features_lr)
sorted_indices_dt = np.argsort(imp_features_dt)
sorted_indices_rf = np.argsort(imp_features_rf)
sorted_indices_ada = np.argsort(imp_features_ada)

fig, axs = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Feature Importance', fontsize=16)

# Nova paleta (clean e cient√≠fica)
colors = [
    "#1f77b4",  # Gradient Boosting
    "#4c72b0",  # Linear Regression
    "#55a868",  # Decision Tree
    "#8172b2",  # Random Forest
    "#c44e52"   # AdaBoost
]

for ax in axs.flat:
    ax.set_facecolor('#f5f5f5')
    ax.grid(True, which='both', axis='x', linestyle='--', alpha=0.35)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)

# PLOT 1: Gradient Boosting
axs[0, 0].barh(
    features[sorted_indices_gb],
    imp_features_gb[sorted_indices_gb],
    color=colors[0]
)
axs[0, 0].set_title('Gradient Boosting Regressor')
axs[0, 0].set_xlabel('Importance')
axs[0, 0].set_ylabel('Feature')

# PLOT 2: Linear Regression
axs[0, 1].barh(
    features[sorted_indices_lr],
    imp_features_lr[sorted_indices_lr],
    color=colors[1]
)
axs[0, 1].set_title('Linear Regression')
axs[0, 1].set_xlabel('Importance')
axs[0, 1].set_ylabel('Feature')

# PLOT 3: Decision Tree
axs[0, 2].barh(
    features[sorted_indices_dt],
    imp_features_dt[sorted_indices_dt],
    color=colors[2]
)
axs[0, 2].set_title('Decision Tree Regressor')
axs[0, 2].set_xlabel('Importance')
axs[0, 2].set_ylabel('Feature')

# PLOT 4: Random Forest
axs[1, 0].barh(
    features[sorted_indices_rf],
    imp_features_rf[sorted_indices_rf],
    color=colors[3]
)
axs[1, 0].set_title('Random Forest Regressor')
axs[1, 0].set_xlabel('Importance')
axs[1, 0].set_ylabel('Feature')

# PLOT 5: AdaBoost
axs[1, 1].barh(
    features[sorted_indices_ada],
    imp_features_ada[sorted_indices_ada],
    color=colors[4]
)
axs[1, 1].set_title('AdaBoost Regressor')
axs[1, 1].set_xlabel('Importance')
axs[1, 1].set_ylabel('Feature')

# Remove subplot vazio
fig.delaxes(axs[1, 2])

plt.tight_layout(rect=[0, 0, 1, 0.95], h_pad=1.5)
plt.savefig('FEATUREIMPORTANCE2.jpg', dpi=600, bbox_inches='tight')
plt.show()